{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":613,"status":"ok","timestamp":1669724322578,"user":{"displayName":"Thesis HCMUT","userId":"14466607749668505114"},"user_tz":-420},"id":"2CFe0-OdIq2e","outputId":"e6cfdff2-b8a6-473c-fd99-a5af3abf94a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Hiep/tsn-online-demo\n"]}],"source":["%cd '/content/drive/MyDrive/Hiep/tsn-online-demo'"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1141,"status":"ok","timestamp":1669724326238,"user":{"displayName":"Thesis HCMUT","userId":"14466607749668505114"},"user_tz":-420},"id":"A5UwWpx8JcJX","outputId":"ac635c3c-9a0f-423b-ae4a-0d3d817daec3"},"outputs":[{"name":"stdout","output_type":"stream","text":["main.py  ops  tsn-online.ipynb\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jvWIBIriJctr","outputId":"e3b30d9f-2d5d-43cd-b69c-75cbdeb48b19"},"outputs":[{"name":"stdout","output_type":"stream","text":["Build transformer...\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:333: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n","  \"Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. \"\n","Build Executor...\n","\n","    Initializing TSN with base model: resnet50.\n","    TSN Configurations:\n","        input_modality:     RGB\n","        num_segments:       8\n","        new_length:         1\n","        consensus_module:   avg\n","        dropout_ratio:      0.8\n","        img_feature_dim:    256\n","            \n","=> base model: resnet50\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:136: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and will be removed in 0.15. Please use keyword parameter(s) instead.\n","  f\"Using {sequence_to_str(tuple(keyword_only_kwargs.keys()), separate_last='and ')} as positional \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Adding temporal shift...\n","=> n_segment per stage: [8, 8, 8, 8]\n","=> Processing stage with 3 blocks residual\n","=> Using fold div: 8\n","=> Using fold div: 8\n","=> Using fold div: 8\n","=> Processing stage with 4 blocks residual\n","=> Using fold div: 8\n","=> Using fold div: 8\n","=> Using fold div: 8\n","=> Using fold div: 8\n","=> Processing stage with 6 blocks residual\n","=> Using fold div: 8\n","=> Using fold div: 8\n","=> Using fold div: 8\n","=> Using fold div: 8\n","=> Using fold div: 8\n","=> Using fold div: 8\n","=> Processing stage with 3 blocks residual\n","=> Using fold div: 8\n","=> Using fold div: 8\n","=> Using fold div: 8\n","=> Load pretrain model from '/content/drive/MyDrive/Action Recognition/TSM MODEL + ASSEMBLY101 DATASET/TSM-action-recognition/checkpoint/TSM_PhonePackaging_RGB_resnet50_shift8_blockres_avg_segment8_e50/ckpt.best.pth.tar'\n","Ready!\n","num_frames:  1\n","[0. 0. 0. 0. 0. 0. 0. 0.]\n","torch.Size([8, 3, 256, 256])\n","torch.Size([1, 23])\n","0.0 put down phone on table\n","num_frames:  2\n","[0. 0. 0. 0. 0. 0. 0. 0.]\n","torch.Size([8, 3, 256, 256])\n","torch.Size([1, 23])\n","0.0 pick up instruction paper from table\n","num_frames:  3\n","[0. 0. 0. 0. 0. 0. 0. 0.]\n","torch.Size([8, 3, 256, 256])\n","torch.Size([1, 23])\n","0.0 pick up instruction paper from table\n","num_frames:  4\n","[0. 0. 0. 0. 0. 0. 0. 0.]\n","torch.Size([8, 3, 256, 256])\n","torch.Size([1, 23])\n","0.0 put down phone on table\n","num_frames:  5\n","[0. 0. 0. 0. 0. 0. 0. 0.]\n","torch.Size([8, 3, 256, 256])\n","torch.Size([1, 23])\n","0.0 pick up instruction paper from table\n","num_frames:  6\n","[0. 0. 0. 0. 0. 0. 0. 0.]\n","torch.Size([8, 3, 256, 256])\n","torch.Size([1, 23])\n","0.0 put down phone on table\n","num_frames:  7\n","[0. 0. 0. 0. 0. 0. 0. 0.]\n","torch.Size([8, 3, 256, 256])\n","torch.Size([1, 23])\n","0.0 pick up phone from table\n","num_frames:  8\n","[0 1 2 3 4 5 6 7]\n","torch.Size([8, 3, 256, 256])\n","torch.Size([1, 23])\n","7 pick up instruction paper from table\n","num_frames:  9\n","[0 1 2 3 4 5 6 7]\n","torch.Size([8, 3, 256, 256])\n","torch.Size([1, 23])\n","7 pick up instruction paper from table\n","num_frames:  10\n","[0 1 2 3 4 5 6 7]\n","torch.Size([8, 3, 256, 256])\n","torch.Size([1, 23])\n","7 pick up instruction paper from table\n","num_frames:  11\n","[0 1 2 3 4 5 6 7]\n","torch.Size([8, 3, 256, 256])\n","torch.Size([1, 23])\n","7 pick up instruction paper from table\n","num_frames:  12\n","[0 1 2 3 4 5 6 7]\n","torch.Size([8, 3, 256, 256])\n","torch.Size([1, 23])\n","7 put down phone on table\n","num_frames:  13\n","[0 1 2 3 4 5 6 7]\n","torch.Size([8, 3, 256, 256])\n","torch.Size([1, 23])\n","7 pick up instruction paper from table\n","num_frames:  14\n","[0 1 2 3 4 5 6 7]\n","torch.Size([8, 3, 256, 256])\n","torch.Size([1, 23])\n","7 pick up instruction paper from table\n","num_frames:  15\n","[0 1 2 3 4 5 6 7]\n","torch.Size([8, 3, 256, 256])\n","torch.Size([1, 23])\n","7 pick up instruction paper from table\n","num_frames:  16\n","[ 1  3  5  6  8 10 13 15]\n","torch.Size([8, 3, 256, 256])\n","torch.Size([1, 23])\n","15 put down phone on table\n","num_frames:  17\n","[ 0  2  4  6  9 10 13 14]\n","torch.Size([8, 3, 256, 256])\n","torch.Size([1, 23])\n","14 put down phone on table\n","num_frames:  18\n","[ 1  3  4  6  8 10 12 15]\n","torch.Size([8, 3, 256, 256])\n","torch.Size([1, 23])\n","15 pick up instruction paper from table\n","num_frames:  19\n","[ 1  2  4  6  8 11 13 14]\n","torch.Size([8, 3, 256, 256])\n","torch.Size([1, 23])\n","14 pick up instruction paper from table\n","num_frames:  20\n","[ 1  3  4  7  8 11 12 15]\n","torch.Size([8, 3, 256, 256])\n","torch.Size([1, 23])\n","15 pick up instruction paper from table\n","num_frames:  21\n","[ 0  2  5  6  8 11 13 15]\n","torch.Size([8, 3, 256, 256])\n","torch.Size([1, 23])\n","15 pick up instruction paper from table\n","num_frames:  22\n","[ 1  3  4  7  9 11 13 14]\n","torch.Size([8, 3, 256, 256])\n","torch.Size([1, 23])\n","14 pick up instruction paper from table\n","num_frames:  23\n","[ 0  2  4  7  8 10 12 15]\n","torch.Size([8, 3, 256, 256])\n","torch.Size([1, 23])\n","15 pick up instruction paper from table\n","num_frames:  24\n","[ 1  5  6  9 14 15 20 21]\n","torch.Size([8, 3, 256, 256])\n","torch.Size([1, 23])\n","21 put down earphones into phone box\n","num_frames:  25\n","[ 1  4  7  9 14 16 18 21]\n","torch.Size([8, 3, 256, 256])\n","torch.Size([1, 23])\n","21 pick up instruction paper from table\n","num_frames:  26\n","[ 0  3  7 10 13 15 19 21]\n","torch.Size([8, 3, 256, 256])\n","torch.Size([1, 23])\n","21 pick up instruction paper from table\n","num_frames:  27\n","[ 0  3  7 11 12 16 18 22]\n","torch.Size([8, 3, 256, 256])\n","torch.Size([1, 23])\n","22 pick up instruction paper from table\n","num_frames:  28\n","[ 2  4  8  9 14 17 20 23]\n","torch.Size([8, 3, 256, 256])\n","torch.Size([1, 23])\n","23 pick up instruction paper from table\n","num_frames:  29\n","[ 1  3  7  9 12 15 19 23]\n","torch.Size([8, 3, 256, 256])\n","torch.Size([1, 23])\n","23 pick up instruction paper from table\n","num_frames:  30\n","[ 2  5  8 11 12 17 19 22]\n","torch.Size([8, 3, 256, 256])\n","torch.Size([1, 23])\n","22 pick up instruction paper from table\n","num_frames:  31\n","[ 1  5  7  9 12 17 20 22]\n","torch.Size([8, 3, 256, 256])\n"]}],"source":["!python main.py"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyO62yIYFSRpQEWTeGFEmv36","mount_file_id":"1d3jOFJmvbiFEOXyXyariEWOPP5Tt3TZp","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.2"},"vscode":{"interpreter":{"hash":"47a088110ad0545c064c9fdc641941baa25b381e8748f361fadc7dd7369fe4e3"}}},"nbformat":4,"nbformat_minor":0}
